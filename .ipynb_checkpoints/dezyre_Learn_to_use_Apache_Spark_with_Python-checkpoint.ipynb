{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic command actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* myRDDdata.collect(): see the content of the RDD data to the Driver Node (which can cause out of memory)\n",
    "* myRDDdata.select(): holds only the columns which were selected\n",
    "* myRDDdata.First(): returns the first element from the dataset\n",
    "* myRDDdata.Take(n): returns the first n lines from the dataset\n",
    "* myRDDdata.TakeSample(withReplacement, n, [seed]): sample n lines from the dataset\n",
    "* myRDDdata.Count(): returns the total number of lines in the dataset\n",
    "* myRDDdata.reduce()\n",
    "* myRDDdata.unpersist(): removes the cached data from memory\n",
    "* myRDDdata.countByValue(): works as value_counts() in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map()\n",
    "* flatMap()\n",
    "* filter()\n",
    "* sample()\n",
    "* union()\n",
    "* intersection()\n",
    "* distinct()\n",
    "* join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelism is the key feature of any distributed system where operations are done by dividing the data into multiple parallel partitions. The same operation is performed on the partitions simultaneously which helps achieve fast data processing with spark. **Map and Reduce** operations can be effectively applied in parallel in apache spark by dividing the data into multiple partitions. A copy of each partition within an RDD is distributed across several workers running on different nodes of a cluster so that in case of failure of a single worker the RDD still remains available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree of parallelism of each operation on RDD depends on **the fixed number of partitions** that an RDD has. We can specify the degree of parallelism or the number of partitions when creating it or later on using the repartition() and coalesce() methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* partRDD = sc.textFile('/opt/spark/CHANGES.txt', 4)\n",
    "* partRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing data with **reduceByKey** operation, Spark will form as many number of output partitions based on the default parallelism which depends on the numbers of nodes and cores available on each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher level --> Lower level: \n",
    "* Clusters --> Nodes --> Cores/Workers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs a map operation individually on each partition, unlike a normal map operation where map is used to operate on each line of the entire RDD.\n",
    "* partRDD.mapPartitions() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample code (in local mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('MyFirstStandalongApp')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "userRDD = sc.textFile(\"/usr/lib/data_backup/opt/spark_usecases/movie/ml-100k/u.user\")\n",
    "\n",
    "def parse_N_calculate_age(data):\n",
    "    userid, age, gender, occupation, zipcode = data.split(\"|\")\n",
    "    return userid, age_group(int(age)), gender, occupation, zipcode, int(age)\n",
    "\n",
    "def age_group(age):\n",
    "    if age < 10:\n",
    "        return \"0-10\"\n",
    "    elif age < 20:\n",
    "        return \"10-20\"\n",
    "    elif age < 30:\n",
    "        return \"20-30\"\n",
    "    elif age < 40:\n",
    "        return \"30-40\"\n",
    "    elif age < 50:\n",
    "        return \"40-50\"\n",
    "    elif age < 60:\n",
    "        return \"50-60\"\n",
    "    elif age < 70:\n",
    "        return \"60-70\"\n",
    "    elif age < 80:\n",
    "        return \"70-80\"\n",
    "    else: \n",
    "        return \"80+\"\n",
    "\n",
    "data_with_age_bucket = userRDD.map(parse_N_calculate_age)\n",
    "\n",
    "RDD_20_30 = data_with_age_bucket.filter(lambda line: \"20-30\" in line)\n",
    "freq = RDD_20_30.map(lambda line: line[3]).countByValue()\n",
    "\n",
    "print(\"total user count is\", userRDD.count())\n",
    "print(\"total movie users profession are\", dict(freq))\n",
    "\n",
    "\n",
    "\n",
    "# Cover the global variables by Accumulator feature to share across tasks\n",
    "Under_age = sc.accumulator(0)\n",
    "Over_age = sc.accumulator(0)\n",
    "\n",
    "def outliers(data):\n",
    "    global Over_age, Under_age # without Accumulator, the global variable is only defined on the driver node\n",
    "    age_grp = data[1]\n",
    "    if age_grp in [\"70-80\",\"80+\"]:\n",
    "        Over_age += 1\n",
    "    elif age_grp == \"0-10\":\n",
    "        Under_age += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "# collect() returns the entire dataset to the Driver Node which can cause out of memory\n",
    "df = data_with_age_bucket.map(outliers).collect()\n",
    "\n",
    "print(\"under age users of the movie are \", Under_age)\n",
    "print(\"over age users of the movie are \", Over_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mylist = pd.Series([1,2,3,4,2,3,4,2,1,2,1,1,2,3,4,3,4,3,2,1,2,1])\n",
    "mydf = mylist.value_counts().sort_index().reset_index()\n",
    "mydf.columns = ['A','B']\n",
    "mydf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Spark on Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apache Hadoop YARN: HDFS is the source storage and YARN is the resource manager in this scenario. All read or write operations in this mode are performed on HDFS.\n",
    "2. Apache Mesos: A distributed mode where the resource management is handled by the cluster manager Apache Mesos developed by UC Berkeley.\n",
    "3. Standalone Mode: In this mode the resource management is handled by the spark in-built resource manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the application in cluster mode you should have your distributed cluster set up already with all the workers listening to the master.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
